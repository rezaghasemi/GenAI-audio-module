{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaghasemi/GenAI-audio-module/blob/main/assignement%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72af2d2f"
      },
      "source": [
        "## WaveNet Model Explanation\n",
        "\n",
        "This notebook implements a WaveNet model, which is a deep generative model for raw audio waveforms. Key components and concepts in this implementation include:\n",
        "\n",
        "- **Dilated Causal Convolutions:** The core of WaveNet. These convolutions have a \"hole\" between the weights, allowing the receptive field to grow exponentially with depth without increasing the number of parameters linearly. This is crucial for capturing long-range dependencies in sequences like audio. The `dilate` function and `DilatedQueue` class likely support this mechanism.\n",
        "- **Gated Activation Units:** Similar to gates in LSTMs, these units control the flow of information through the network, allowing it to selectively remember or forget information. The `filter_convs` and `gate_convs` in the `WaveNetModel` class implement this.\n",
        "- **Residual and Skip Connections:** These connections help to train deeper networks by providing alternative paths for gradients to flow. Residual connections add the output of a dilated convolution block to its input, while skip connections contribute to the final output layer.\n",
        "- **Softmax Output:** The model outputs a probability distribution over possible next values in the audio waveform. This allows for generating diverse and realistic audio.\n",
        "- **Mu-law Compounding:** A technique used to represent audio signals with a non-linear quantization, which is particularly effective for low-amplitude signals. The `mu_law` function implements this.\n",
        "- **Fast Generation:** The `generate_fast` method likely utilizes the `DilatedQueue` to speed up the generation process by avoiding redundant computations.\n",
        "\n",
        "The `WaveNetModel` class combines these components into a neural network architecture capable of learning the underlying structure of audio data and generating new samples."
      ],
      "id": "72af2d2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a8253bf"
      },
      "source": [
        "In this assignment, you will learn how to:\n",
        "\n",
        "1. **Manipulate data:** Prepare and process audio data for the WaveNet model.\n",
        "2. **Train the model:** Understand the training process and train the WaveNet model on your data.\n",
        "3. **Generate Audio:** Use the trained model to generate new audio samples."
      ],
      "id": "7a8253bf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1d7b8",
      "metadata": {
        "id": "dee1d7b8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable, Function\n",
        "from scipy.io import wavfile\n",
        "import time\n",
        "import os\n",
        "import librosa as lr\n",
        "from torch.utils.data import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0778602"
      },
      "source": [
        "The `mu_law_compand` and `quantize` functions applies µ-law and quantization to an audio signal.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "- **µ-law companding:** This is a non-linear process that compresses the dynamic range of an audio signal. It gives more resolution to lower amplitude values and less resolution to higher amplitude values. This is particularly useful for audio as the human ear is more sensitive to changes in quiet sounds than loud sounds.\n",
        "- **Quantization:** After companding, the function quantizes the signal to a specified number of levels (defaulting to 256). This converts the continuous audio signal into a discrete representation, which is necessary for the WaveNet model's output layer (which predicts the probability of the next discrete audio value).\n",
        "\n",
        "Essentially, this function prepares the raw audio data for the WaveNet model by applying a transformation that is perceptually motivated and converts the data into a format suitable for the model's output layer."
      ],
      "id": "b0778602"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def mu_law_compand(x, mu=256):\n",
        "    \"\"\"µ-law companding: [-1,1] -> [-1,1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    safe_x = np.clip(x, -1.0, 1.0)\n",
        "    fx = np.sign(safe_x) * np.log1p(mu * np.abs(safe_x)) / np.log1p(mu)\n",
        "    return fx\n",
        "\n",
        "def quantize(fx, mu=256):\n",
        "    \"\"\"Quantize companded signal: [-1,1] -> [0, mu-1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    return ((fx + 1) / 2 * mu + 0.5).astype(np.int32)\n",
        "\n",
        "def dequantize(q, mu=256):\n",
        "    \"\"\"Inverse quantization: [0, mu-1] -> [-1,1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    return 2 * q.astype(np.float32) / mu - 1\n",
        "\n",
        "def mu_law_expand(fx, mu=256):\n",
        "    \"\"\"µ-law expansion: [-1,1] -> [-1,1] (approx inverse of compand).\"\"\"\n",
        "    mu = mu - 1\n",
        "    return np.sign(fx) * (np.exp(np.abs(fx) * np.log(mu + 1)) - 1) / mu"
      ],
      "metadata": {
        "id": "aJimah4Ca5wG"
      },
      "id": "aJimah4Ca5wG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa118c3f"
      },
      "source": [
        "## WavenetDataset Class\n",
        "\n",
        "The `WavenetDataset` class is a custom PyTorch `Dataset` designed to handle audio data for the WaveNet model. It prepares the data in a format suitable for training.\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - Takes the folder path containing WAV files, `item_length` (receptive field size), `target_length` (number of samples to predict), sampling rate (`sr`), number of quantization classes (`classes`), and a `normalize` flag.\n",
        "    - Loads all WAV files from the specified folder.\n",
        "    - Applies µ-law encoding (using the `mu_law_encoding` function) to each audio file.\n",
        "    - Concatenates all processed audio data into a single NumPy array (`self.data`).\n",
        "\n",
        "- **Length (`__len__`)**:\n",
        "    - Returns the total number of possible training examples in the dataset. This is calculated by subtracting the `item_length` and `target_length` from the total length of the concatenated audio data.\n",
        "\n",
        "- **Get Item (`__getitem__`)**:\n",
        "    - Takes an index `idx` and returns a single training example.\n",
        "    - Extracts a segment of length `item_length` as the input (`x`) and the subsequent `target_length` segment as the target (`y`).\n",
        "    - Converts the input `x` into a one-hot encoded tensor, which is the format expected by the WaveNet model's input layer.\n",
        "    - Converts the target `y` into a PyTorch LongTensor.\n",
        "    - Returns the one-hot encoded input and the target tensor."
      ],
      "id": "fa118c3f"
    },
    {
      "cell_type": "code",
      "source": [
        "class WavenetDataset(Dataset):\n",
        "    def __init__(self,\n",
        "                 folder,\n",
        "                 item_length=1600,\n",
        "                 target_length=1,\n",
        "                 sr=16000,\n",
        "                 classes=256,\n",
        "                 normalize=False\n",
        "                 ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            folder (str): Path to folder with audio files (.wav).\n",
        "            item_length (int): Input receptive field size.\n",
        "            target_length (int): Number of future samples to predict.\n",
        "            sr (int): Sampling rate for audio.\n",
        "            classes (int): µ-law quantization classes.\n",
        "            normalize (bool): Normalize audio amplitude.\n",
        "        \"\"\"\n",
        "        self.item_length = item_length\n",
        "        self.target_length = target_length\n",
        "        self.classes = classes\n",
        "        self.sr = sr\n",
        "        self.normalize = normalize\n",
        "\n",
        "        # load and preprocess each file into a list of arrays\n",
        "        self.data = []\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".wav\"):\n",
        "                y, _ = lr.load(os.path.join(folder, fname), sr=self.sr, mono=True)\n",
        "                if self.normalize:\n",
        "                    y = lr.util.normalize(y)\n",
        "                q = mu_law_compand(y, classes)\n",
        "                q = quantize(q, classes)\n",
        "                self.data.append(q)\n",
        "\n",
        "        # for indexing: store (file_index, start_position) pairs\n",
        "        self.index_map = []\n",
        "        for f_idx, arr in enumerate(self.data):\n",
        "            length = len(arr)\n",
        "            max_start = length - (self.item_length + self.target_length)\n",
        "            for start in range(max_start):\n",
        "                self.index_map.append((f_idx, start))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.index_map)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        f_idx, start = self.index_map[idx]\n",
        "        arr = self.data[f_idx]\n",
        "\n",
        "        x = arr[start : start + self.item_length]\n",
        "        y = arr[start + self.item_length : start + self.item_length + self.target_length]\n",
        "\n",
        "        # one-hot input (classes x item_length)\n",
        "        x_onehot = torch.zeros(self.classes, self.item_length)\n",
        "        x_tensor = torch.from_numpy(x).long()\n",
        "        x_onehot.scatter_(0, x_tensor.unsqueeze(0), 1.0)\n",
        "\n",
        "        y = torch.from_numpy(y).long()\n",
        "        return x_onehot, y\n"
      ],
      "metadata": {
        "id": "RgD7_vb4fVWA"
      },
      "id": "RgD7_vb4fVWA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4450bed7",
      "metadata": {
        "id": "4450bed7"
      },
      "outputs": [],
      "source": [
        "\n",
        "def dilate(x, dilation, init_dilation=1, pad_start=True):\n",
        "    \"\"\"\n",
        "    :param x: Tensor of size (N, C, L), where N is the input dilation, C is the number of channels, and L is the input length\n",
        "    :param dilation: Target dilation. Will be the size of the first dimension of the output tensor.\n",
        "    :param pad_start: If the input length is not compatible with the specified dilation, zero padding is used. This parameter determines wether the zeros are added at the start or at the end.\n",
        "    :return: The dilated tensor of size (dilation, C, L*N / dilation). The output might be zero padded at the start\n",
        "    \"\"\"\n",
        "\n",
        "    [n, c, l] = x.size()\n",
        "    dilation_factor = dilation / init_dilation\n",
        "    if dilation_factor == 1:\n",
        "        return x\n",
        "\n",
        "    # zero padding for reshaping\n",
        "    new_l = int(np.ceil(l / dilation_factor) * dilation_factor)\n",
        "    if new_l != l:\n",
        "        l = new_l\n",
        "        x = constant_pad_1d(x, new_l, dimension=2, pad_start=pad_start)\n",
        "\n",
        "    l_old = int(round(l / dilation_factor))\n",
        "    n_old = int(round(n * dilation_factor))\n",
        "    l = math.ceil(l * init_dilation / dilation)\n",
        "    n = math.ceil(n * dilation / init_dilation)\n",
        "\n",
        "    # reshape according to dilation\n",
        "    x = x.permute(1, 2, 0).contiguous()  # (n, c, l) -> (c, l, n)\n",
        "    x = x.view(c, l, n)\n",
        "    x = x.permute(2, 0, 1).contiguous()  # (c, l, n) -> (n, c, l)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "class DilatedQueue:\n",
        "    def __init__(self, max_length, data=None, dilation=1, num_deq=1, num_channels=1, dtype=torch.FloatTensor):\n",
        "        self.in_pos = 0\n",
        "        self.out_pos = 0\n",
        "        self.num_deq = num_deq\n",
        "        self.num_channels = num_channels\n",
        "        self.dilation = dilation\n",
        "        self.max_length = max_length\n",
        "        self.data = data\n",
        "        self.dtype = dtype\n",
        "        if data == None:\n",
        "            self.data = Variable(dtype(num_channels, max_length).zero_())\n",
        "\n",
        "    def enqueue(self, input):\n",
        "        self.data[:, self.in_pos] = input.view(-1)\n",
        "        self.in_pos = (self.in_pos + 1) % self.max_length\n",
        "\n",
        "    def dequeue(self, num_deq=1, dilation=1):\n",
        "        #       |\n",
        "        #  |6|7|8|1|2|3|4|5|\n",
        "        #         |\n",
        "        start = self.out_pos - ((num_deq - 1) * dilation)\n",
        "        if start < 0:\n",
        "            t1 = self.data[:, start::dilation]\n",
        "            t2 = self.data[:, self.out_pos % dilation:self.out_pos + 1:dilation]\n",
        "            t = torch.cat((t1, t2), 1)\n",
        "        else:\n",
        "            t = self.data[:, start:self.out_pos + 1:dilation]\n",
        "\n",
        "        self.out_pos = (self.out_pos + 1) % self.max_length\n",
        "        return t\n",
        "\n",
        "    def reset(self):\n",
        "        self.dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
        "        self.data = Variable(self.dtype(self.num_channels, self.max_length).zero_())\n",
        "        self.in_pos = 0\n",
        "        self.out_pos = 0\n",
        "\n",
        "\n",
        "class ConstantPad1d(Function):\n",
        "    def __init__(self, target_size, dimension=0, value=0, pad_start=False):\n",
        "        super(ConstantPad1d, self).__init__()\n",
        "        self.target_size = target_size\n",
        "        self.dimension = dimension\n",
        "        self.value = value\n",
        "        self.pad_start = pad_start\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.num_pad = self.target_size - input.size(self.dimension)\n",
        "        assert self.num_pad >= 0, 'target size has to be greater than input size'\n",
        "\n",
        "        self.input_size = input.size()\n",
        "\n",
        "        size = list(input.size())\n",
        "        size[self.dimension] = self.target_size\n",
        "        output = input.new(*tuple(size)).fill_(self.value)\n",
        "        c_output = output\n",
        "\n",
        "        # crop output\n",
        "        if self.pad_start:\n",
        "            c_output = c_output.narrow(self.dimension, self.num_pad, c_output.size(self.dimension) - self.num_pad)\n",
        "        else:\n",
        "            c_output = c_output.narrow(self.dimension, 0, c_output.size(self.dimension) - self.num_pad)\n",
        "\n",
        "        c_output.copy_(input)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        grad_input = grad_output.new(*self.input_size).zero_()\n",
        "        cg_output = grad_output\n",
        "\n",
        "        # crop grad_output\n",
        "        if self.pad_start:\n",
        "            cg_output = cg_output.narrow(self.dimension, self.num_pad, cg_output.size(self.dimension) - self.num_pad)\n",
        "        else:\n",
        "            cg_output = cg_output.narrow(self.dimension, 0, cg_output.size(self.dimension) - self.num_pad)\n",
        "\n",
        "        grad_input.copy_(cg_output)\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "def constant_pad_1d(input,\n",
        "                    target_size,\n",
        "                    dimension=0,\n",
        "                    value=0,\n",
        "                    pad_start=False):\n",
        "    return ConstantPad1d(target_size, dimension, value, pad_start)(input)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace484f3",
      "metadata": {
        "id": "ace484f3"
      },
      "outputs": [],
      "source": [
        "class WaveNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Complete Wavenet Model\n",
        "\n",
        "    Args:\n",
        "        layers (Int):               Number of layers in each block\n",
        "        blocks (Int):               Number of wavenet blocks of this model\n",
        "        dilation_channels (Int):    Number of channels for the dilated convolution\n",
        "        residual_channels (Int):    Number of channels for the residual connection\n",
        "        skip_channels (Int):        Number of channels for the skip connections\n",
        "        classes (Int):              Number of possible values each sample can have\n",
        "        output_length (Int):        Number of samples that are generated for each input\n",
        "        kernel_size (Int):          Size of the dilation kernel\n",
        "        dtype:                      Parameter type of this model\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C_{in}, L_{in})`\n",
        "        - Output: :math:`()`\n",
        "        L should be the length of the receptive field\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 layers=10,\n",
        "                 blocks=4,\n",
        "                 dilation_channels=32,\n",
        "                 residual_channels=32,\n",
        "                 skip_channels=256,\n",
        "                 end_channels=256,\n",
        "                 classes=256,\n",
        "                 output_length=32,\n",
        "                 kernel_size=2,\n",
        "                 dtype=torch.FloatTensor,\n",
        "                 bias=False):\n",
        "\n",
        "        super(WaveNetModel, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.blocks = blocks\n",
        "        self.dilation_channels = dilation_channels\n",
        "        self.residual_channels = residual_channels\n",
        "        self.skip_channels = skip_channels\n",
        "        self.classes = classes\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # build model\n",
        "        receptive_field = 1\n",
        "        init_dilation = 1\n",
        "\n",
        "        self.dilations = []\n",
        "        self.dilated_queues = []\n",
        "        # self.main_convs = nn.ModuleList()\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        # 1x1 convolution to create channels\n",
        "        self.start_conv = nn.Conv1d(in_channels=self.classes,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=1,\n",
        "                                    bias=bias)\n",
        "\n",
        "        for b in range(blocks):\n",
        "            additional_scope = kernel_size - 1\n",
        "            new_dilation = 1\n",
        "            for i in range(layers):\n",
        "                # dilations of this layer\n",
        "                self.dilations.append((new_dilation, init_dilation))\n",
        "\n",
        "                # dilated queues for fast generation\n",
        "                self.dilated_queues.append(DilatedQueue(max_length=(kernel_size - 1) * new_dilation + 1,\n",
        "                                                        num_channels=residual_channels,\n",
        "                                                        dilation=new_dilation,\n",
        "                                                        dtype=dtype))\n",
        "\n",
        "                # dilated convolutions\n",
        "                self.filter_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                                   out_channels=dilation_channels,\n",
        "                                                   kernel_size=kernel_size,\n",
        "                                                   bias=bias))\n",
        "\n",
        "                self.gate_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                                 out_channels=dilation_channels,\n",
        "                                                 kernel_size=kernel_size,\n",
        "                                                 bias=bias))\n",
        "\n",
        "                # 1x1 convolution for residual connection\n",
        "                self.residual_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
        "                                                     out_channels=residual_channels,\n",
        "                                                     kernel_size=1,\n",
        "                                                     bias=bias))\n",
        "\n",
        "                # 1x1 convolution for skip connection\n",
        "                self.skip_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
        "                                                 out_channels=skip_channels,\n",
        "                                                 kernel_size=1,\n",
        "                                                 bias=bias))\n",
        "\n",
        "                receptive_field += additional_scope\n",
        "                additional_scope *= 2\n",
        "                init_dilation = new_dilation\n",
        "                new_dilation *= 2\n",
        "\n",
        "        self.end_conv_1 = nn.Conv1d(in_channels=skip_channels,\n",
        "                                  out_channels=end_channels,\n",
        "                                  kernel_size=1,\n",
        "                                  bias=True)\n",
        "\n",
        "        self.end_conv_2 = nn.Conv1d(in_channels=end_channels,\n",
        "                                    out_channels=classes,\n",
        "                                    kernel_size=1,\n",
        "                                    bias=True)\n",
        "\n",
        "        # self.output_length = 2 ** (layers - 1)\n",
        "        self.output_length = output_length\n",
        "        self.receptive_field = receptive_field\n",
        "\n",
        "    def wavenet(self, input, dilation_func):\n",
        "\n",
        "        x = self.start_conv(input)\n",
        "        skip = 0\n",
        "\n",
        "        # WaveNet layers\n",
        "        for i in range(self.blocks * self.layers):\n",
        "\n",
        "            #            |----------------------------------------|     *residual*\n",
        "            #            |                                        |\n",
        "            #            |    |-- conv -- tanh --|                |\n",
        "            # -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*\n",
        "            #                 |-- conv -- sigm --|     |\n",
        "            #                                         1x1\n",
        "            #                                          |\n",
        "            # ---------------------------------------> + ------------->\t*skip*\n",
        "\n",
        "            (dilation, init_dilation) = self.dilations[i]\n",
        "\n",
        "            residual = dilation_func(x, dilation, init_dilation, i)\n",
        "\n",
        "            # dilated convolution\n",
        "            filter = self.filter_convs[i](residual)\n",
        "            filter = F.tanh(filter)\n",
        "            gate = self.gate_convs[i](residual)\n",
        "            gate = F.sigmoid(gate)\n",
        "            x = filter * gate\n",
        "\n",
        "            # parametrized skip connection\n",
        "            s = x\n",
        "            if x.size(2) != 1:\n",
        "                 s = dilate(x, 1, init_dilation=dilation)\n",
        "            s = self.skip_convs[i](s)\n",
        "            try:\n",
        "                skip = skip[:, :, -s.size(2):]\n",
        "            except:\n",
        "                skip = 0\n",
        "            skip = s + skip\n",
        "\n",
        "            x = self.residual_convs[i](x)\n",
        "            x = x + residual[:, :, (self.kernel_size - 1):]\n",
        "\n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def wavenet_dilate(self, input, dilation, init_dilation, i):\n",
        "        x = dilate(input, dilation, init_dilation)\n",
        "        return x\n",
        "\n",
        "    def queue_dilate(self, input, dilation, init_dilation, i):\n",
        "        queue = self.dilated_queues[i]\n",
        "        queue.enqueue(input.data[0])\n",
        "        x = queue.dequeue(num_deq=self.kernel_size,\n",
        "                          dilation=dilation)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.wavenet(input,\n",
        "                         dilation_func=self.wavenet_dilate)\n",
        "\n",
        "        # reshape output\n",
        "        [n, c, l] = x.size()\n",
        "        l = self.output_length\n",
        "        x = x[:, :, -l:]\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        x = x.view(n * l, c)\n",
        "        return x\n",
        "\n",
        "    def generate(self,\n",
        "                 num_samples,\n",
        "                 first_samples=None,\n",
        "                 temperature=1.):\n",
        "        self.eval()\n",
        "        if first_samples is None:\n",
        "            first_samples = self.dtype(1).zero_()\n",
        "        generated = Variable(first_samples, volatile=True)\n",
        "\n",
        "        num_pad = self.receptive_field - generated.size(0)\n",
        "        if num_pad > 0:\n",
        "            generated = constant_pad_1d(generated, self.scope, pad_start=True)\n",
        "            print(\"pad zero\")\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            input = Variable(torch.FloatTensor(1, self.classes, self.receptive_field).zero_())\n",
        "            input = input.scatter_(1, generated[-self.receptive_field:].view(1, -1, self.receptive_field), 1.)\n",
        "\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.wavenet_dilate)[:, :, -1].squeeze()\n",
        "\n",
        "            if temperature > 0:\n",
        "                x /= temperature\n",
        "                prob = F.softmax(x, dim=0)\n",
        "                prob = prob.cpu()\n",
        "                np_prob = prob.data.numpy()\n",
        "                x = np.random.choice(self.classes, p=np_prob)\n",
        "                x = Variable(torch.LongTensor([x]))#np.array([x])\n",
        "            else:\n",
        "                x = torch.max(x, 0)[1].float()\n",
        "\n",
        "            generated = torch.cat((generated, x), 0)\n",
        "\n",
        "        generated = (generated / self.classes) * 2. - 1\n",
        "        mu_gen = mu_law_expand(generated, self.classes)\n",
        "\n",
        "        self.train()\n",
        "        return mu_gen\n",
        "\n",
        "    def generate_fast(self,\n",
        "                      num_samples,\n",
        "                      first_samples=None,\n",
        "                      temperature=1.,\n",
        "                      regularize=0.,\n",
        "                      progress_callback=None,\n",
        "                      progress_interval=100):\n",
        "        self.eval()\n",
        "        if first_samples is None:\n",
        "            first_samples = torch.LongTensor(1).zero_() + (self.classes // 2)\n",
        "        first_samples = Variable(first_samples)\n",
        "\n",
        "        # reset queues\n",
        "        for queue in self.dilated_queues:\n",
        "            queue.reset()\n",
        "\n",
        "        num_given_samples = first_samples.size(0)\n",
        "        total_samples = num_given_samples + num_samples\n",
        "\n",
        "        input = Variable(torch.FloatTensor(1, self.classes, 1).zero_())\n",
        "        input = input.scatter_(1, first_samples[0:1].view(1, -1, 1), 1.)\n",
        "\n",
        "        # fill queues with given samples\n",
        "        for i in range(num_given_samples - 1):\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.queue_dilate)\n",
        "            input.zero_()\n",
        "            input = input.scatter_(1, first_samples[i + 1:i + 2].view(1, -1, 1), 1.).view(1, self.classes, 1)\n",
        "\n",
        "            # progress feedback\n",
        "            if i % progress_interval == 0:\n",
        "                if progress_callback is not None:\n",
        "                    progress_callback(i, total_samples)\n",
        "\n",
        "        # generate new samples\n",
        "        generated = np.array([])\n",
        "        regularizer = torch.pow(Variable(torch.arange(self.classes)) - self.classes / 2., 2)\n",
        "        regularizer = regularizer.squeeze() * regularize\n",
        "        tic = time.time()\n",
        "        for i in range(num_samples):\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.queue_dilate).squeeze()\n",
        "\n",
        "            x -= regularizer\n",
        "\n",
        "            if temperature > 0:\n",
        "                # sample from softmax distribution\n",
        "                x /= temperature\n",
        "                prob = F.softmax(x, dim=0)\n",
        "                prob = prob.cpu()\n",
        "                np_prob = prob.data.numpy()\n",
        "                x = np.random.choice(self.classes, p=np_prob)\n",
        "                x = np.array([x])\n",
        "            else:\n",
        "                # convert to sample value\n",
        "                x = torch.max(x, 0)[1][0]\n",
        "                x = x.cpu()\n",
        "                x = x.data.numpy()\n",
        "\n",
        "            o = (x / self.classes) * 2. - 1\n",
        "            generated = np.append(generated, o)\n",
        "\n",
        "            # set new input\n",
        "            x = Variable(torch.from_numpy(x).type(torch.LongTensor))\n",
        "            input.zero_()\n",
        "            input = input.scatter_(1, x.view(1, -1, 1), 1.).view(1, self.classes, 1)\n",
        "\n",
        "            if (i+1) == 100:\n",
        "                toc = time.time()\n",
        "                print(\"one generating step does take approximately \" + str((toc - tic) * 0.01) + \" seconds)\")\n",
        "\n",
        "            # progress feedback\n",
        "            if (i + num_given_samples) % progress_interval == 0:\n",
        "                if progress_callback is not None:\n",
        "                    progress_callback(i + num_given_samples, total_samples)\n",
        "\n",
        "        self.train()\n",
        "        mu_gen = mu_law(generated, self.classes)\n",
        "        wavfile.write(\"generated.wav\", 16000, mu_gen)\n",
        "        return  mu_gen\n",
        "\n",
        "\n",
        "    def parameter_count(self):\n",
        "        par = list(self.parameters())\n",
        "        s = sum([np.prod(list(d.size())) for d in par])\n",
        "        return s\n",
        "\n",
        "    def cpu(self, type=torch.FloatTensor):\n",
        "        self.dtype = type\n",
        "        for q in self.dilated_queues:\n",
        "            q.dtype = self.dtype\n",
        "        super().cpu()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b01d740f",
      "metadata": {
        "id": "b01d740f"
      },
      "outputs": [],
      "source": [
        "model = WaveNetModel(layers=10,\n",
        "                     blocks=3,\n",
        "                     dilation_channels=32,\n",
        "                     residual_channels=32,\n",
        "                     skip_channels=1024,\n",
        "                     end_channels=512,\n",
        "                     output_length=16,\n",
        "                     dtype=torch.FloatTensor,\n",
        "                     bias=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7957e0",
      "metadata": {
        "id": "5d7957e0",
        "outputId": "bca4147b-9343-482f-b7f1-caadab7a11e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint_path = \"checkpoints/trained_model.pth\"\n",
        "\n",
        "# Load checkpoint (it might contain 'state_dict' or the whole model)\n",
        "torch.serialization.add_safe_globals([WaveNetModel])\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea7c6a5",
      "metadata": {
        "id": "4ea7c6a5",
        "outputId": "51674fbf-bbd6-4ed4-b283-005a67fc9d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "one generating step does take approximately 0.006373369693756104 seconds)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 0.00266565,  0.00295683, -0.00054255, ...,  0.09697959,\n",
              "        0.10611724,  0.08467614], shape=(10000,))"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate_fast(10000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}