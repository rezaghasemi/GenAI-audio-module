{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "94022ba5",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaghasemi/GenAI-audio-module/blob/main/assignement%201.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72af2d2f",
      "metadata": {
        "id": "72af2d2f"
      },
      "source": [
        "## WaveNet Model Explanation\n",
        "\n",
        "This notebook implements a simple version of the WaveNet model, which is a deep generative model for raw audio waveforms. \n",
        "\n",
        "The `WaveNetModel` class combines these components into a neural network architecture capable of learning the underlying structure of audio data and generating new samples."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a8253bf",
      "metadata": {
        "id": "7a8253bf"
      },
      "source": [
        "In this assignment, you will learn how to:\n",
        "\n",
        "1. **Manipulate data:** Prepare and process audio data for the WaveNet model.\n",
        "2. **Train the model:** Understand the training process and train the WaveNet model on your data.\n",
        "3. **Generate Audio:** Use the trained model to generate new audio samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dee1d7b8",
      "metadata": {
        "id": "dee1d7b8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import numpy as np\n",
        "from torch.autograd import Variable, Function\n",
        "from scipy.io import wavfile\n",
        "import time\n",
        "import os\n",
        "import librosa as lr\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0778602",
      "metadata": {
        "id": "b0778602"
      },
      "source": [
        "The `mu_law_compand` and `quantize` functions applies µ-law and quantization to an audio signal.\n",
        "\n",
        "> Add blockquote\n",
        "\n",
        "\n",
        "\n",
        "- **µ-law companding:** This is a non-linear process that compresses the dynamic range of an audio signal. It gives more resolution to lower amplitude values and less resolution to higher amplitude values. This is particularly useful for audio as the human ear is more sensitive to changes in quiet sounds than loud sounds.\n",
        "- **Quantization:** After companding, the function quantizes the signal to a specified number of levels (defaulting to 256). This converts the continuous audio signal into a discrete representation, which is necessary for the WaveNet model's output layer (which predicts the probability of the next discrete audio value).\n",
        "\n",
        "Essentially, this function prepares the raw audio data for the WaveNet model by applying a transformation that is perceptually motivated and converts the data into a format suitable for the model's output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aJimah4Ca5wG",
      "metadata": {
        "id": "aJimah4Ca5wG"
      },
      "outputs": [],
      "source": [
        "\n",
        "def mu_law_compand(x, mu=256):\n",
        "    \"\"\"µ-law companding: [-1,1] -> [-1,1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    safe_x = np.clip(x, -1.0, 1.0)\n",
        "    fx = np.sign(safe_x) * np.log1p(mu * np.abs(safe_x)) / np.log1p(mu)\n",
        "    return fx\n",
        "\n",
        "def quantize(fx, mu=256):\n",
        "    \"\"\"Quantize companded signal: [-1,1] -> [0, mu-1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    return ((fx + 1) / 2 * mu + 0.5).astype(np.int32)\n",
        "\n",
        "def dequantize(q, mu=256):\n",
        "    \"\"\"Inverse quantization: [0, mu-1] -> [-1,1].\"\"\"\n",
        "    mu = mu - 1\n",
        "    return 2 * q.astype(np.float32) / mu - 1\n",
        "\n",
        "def mu_law_expand(fx, mu=256):\n",
        "    \"\"\"µ-law expansion: [-1,1] -> [-1,1] (approx inverse of compand).\"\"\"\n",
        "    mu = mu - 1\n",
        "    return np.sign(fx) * (np.exp(np.abs(fx) * np.log(mu + 1)) - 1) / mu"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa118c3f",
      "metadata": {
        "id": "fa118c3f"
      },
      "source": [
        "## WavenetDataset Class\n",
        "\n",
        "The `WavenetDataset` class is a custom PyTorch `Dataset` designed to handle audio data for the WaveNet model. It prepares the data in a format suitable for training.\n",
        "\n",
        "- **Initialization (`__init__`)**:\n",
        "    - Takes the folder path containing WAV files, `item_length` (receptive field size), `target_length` (number of samples to predict), sampling rate (`sr`), number of quantization classes (`classes`), and a `normalize` flag.\n",
        "    - Loads all WAV files from the specified folder.\n",
        "    - Applies µ-law encoding (using the `mu_law_compand` function) to each audio file.\n",
        "    - Quantize each file using `quantize`\n",
        "    - Concatenates all processed audio data into a single NumPy array (`self.data`).\n",
        "\n",
        "- **Length (`__len__`)**:\n",
        "    - Returns the total number of possible training examples in the dataset. This is calculated by subtracting the `item_length` and `target_length` from the total length of the concatenated audio data.\n",
        "\n",
        "- **Get Item (`__getitem__`)**:\n",
        "    - Takes an index `idx` and returns a single training example.\n",
        "    - Converts the input `x` into a one-hot encoded tensor, which is the format expected by the WaveNet model's input layer.\n",
        "    - Converts the target `y` into a PyTorch LongTensor.\n",
        "    - Returns the one-hot encoded input and the target tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RgD7_vb4fVWA",
      "metadata": {
        "id": "RgD7_vb4fVWA"
      },
      "outputs": [],
      "source": [
        "class WavenetDataset(Dataset):\n",
        "    def __init__(self, folder, item_length=32000, target_length=1, sr=16000, classes=256, normalize=False):\n",
        "        self.item_length = item_length\n",
        "        self.target_length = target_length\n",
        "        self.classes = classes\n",
        "        self.sr = sr\n",
        "        self.normalize = normalize\n",
        "\n",
        "        all_data = []\n",
        "        for fname in os.listdir(folder):\n",
        "            if fname.endswith(\".wav\"):\n",
        "                y, _ = lr.load(os.path.join(folder, fname), sr=self.sr, mono=True)\n",
        "                if self.normalize:\n",
        "                    y = lr.util.normalize(y)\n",
        "                q = mu_law_compand(y, classes)\n",
        "                q = quantize(q, classes)\n",
        "                all_data.append(q)\n",
        "\n",
        "        self.data = np.concatenate(all_data, axis=0)\n",
        "        self.num_examples = len(self.data) - (self.item_length + self.target_length)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_examples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx : idx + self.item_length]\n",
        "        y = self.data[idx + self.item_length : idx + self.item_length + self.target_length]\n",
        "\n",
        "        # Convert x to LongTensor (indices)\n",
        "        x_tensor = torch.from_numpy(x).long()\n",
        "\n",
        "        # Convert y to LongTensor\n",
        "        y_tensor = torch.from_numpy(y).long()\n",
        "\n",
        "        # One-hot encode x: shape (classes, item_length)\n",
        "        x_onehot = torch.nn.functional.one_hot(x_tensor, num_classes=self.classes).permute(1, 0).float()\n",
        "\n",
        "        return x_onehot, y_tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40e23161",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = WavenetDataset(folder=\"wavs\")\n",
        "\n",
        "print('Dataset length: ', len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19da0ef0",
      "metadata": {},
      "source": [
        "This part of the code provides the low-level tensor operations and data structures required to efficiently implement dilated convolutions and autoregressive generation in WaveNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4450bed7",
      "metadata": {
        "id": "4450bed7"
      },
      "outputs": [],
      "source": [
        "def constant_pad_1d(input, target_size, dimension=0, value=0, pad_start=False):\n",
        "    \"\"\"\n",
        "    Pads input tensor along `dimension` to target_size using a constant value.\n",
        "    \"\"\"\n",
        "    current_size = input.size(dimension)\n",
        "    num_pad = target_size - current_size\n",
        "    if num_pad <= 0:\n",
        "        return input\n",
        "\n",
        "    # Prepare padding tuple for F.pad (PyTorch expects [pad_left, pad_right] for last dim)\n",
        "    pad = [0] * (2 * input.dim())\n",
        "    if pad_start:\n",
        "        pad[-(2*dimension + 2)] = num_pad  # pad start\n",
        "    else:\n",
        "        pad[-(2*dimension + 1)] = num_pad  # pad end\n",
        "\n",
        "    return F.pad(input, pad, \"constant\", value)\n",
        "\n",
        "\n",
        "def dilate(x, dilation, init_dilation=1, pad_start=True):\n",
        "    \"\"\"\n",
        "    Dilates a tensor along the last dimension, adding padding if necessary.\n",
        "    x: Tensor of shape (N, C, L)\n",
        "    \"\"\"\n",
        "    [n, c, l] = x.size()\n",
        "    dilation_factor = dilation / init_dilation\n",
        "    if dilation_factor == 1:\n",
        "        return x\n",
        "\n",
        "    # zero padding for reshaping\n",
        "    new_l = int(np.ceil(l / dilation_factor) * dilation_factor)\n",
        "    if new_l != l:\n",
        "        x = constant_pad_1d(x, new_l, dimension=2, pad_start=pad_start)\n",
        "        l = new_l\n",
        "\n",
        "    l_old = int(round(l / dilation_factor))\n",
        "    n_old = int(round(n * dilation_factor))\n",
        "    l = math.ceil(l * init_dilation / dilation)\n",
        "    n = math.ceil(n * dilation / init_dilation)\n",
        "\n",
        "    # reshape according to dilation\n",
        "    x = x.permute(1, 2, 0).contiguous()  # (C, L, N)\n",
        "    x = x.view(c, l, n)\n",
        "    x = x.permute(2, 0, 1).contiguous()  # (N, C, L)\n",
        "    return x\n",
        "\n",
        "\n",
        "class DilatedQueue:\n",
        "    def __init__(self, max_length, data=None, dilation=1, num_deq=1, num_channels=1, dtype=torch.FloatTensor):\n",
        "        self.in_pos = 0\n",
        "        self.out_pos = 0\n",
        "        self.num_deq = num_deq\n",
        "        self.num_channels = num_channels\n",
        "        self.dilation = dilation\n",
        "        self.max_length = max_length\n",
        "        self.data = data\n",
        "        self.dtype = dtype\n",
        "        if data is None:\n",
        "            self.data = torch.zeros(num_channels, max_length, dtype=dtype)\n",
        "\n",
        "\n",
        "    def enqueue(self, input):\n",
        "        self.data[:, self.in_pos] = input.view(-1)\n",
        "        self.in_pos = (self.in_pos + 1) % self.max_length\n",
        "\n",
        "    def dequeue(self, num_deq=1, dilation=1):\n",
        "        start = self.out_pos - ((num_deq - 1) * dilation)\n",
        "        if start < 0:\n",
        "            t1 = self.data[:, start::dilation]\n",
        "            t2 = self.data[:, self.out_pos % dilation:self.out_pos + 1:dilation]\n",
        "            t = torch.cat((t1, t2), 1)\n",
        "        else:\n",
        "            t = self.data[:, start:self.out_pos + 1:dilation]\n",
        "\n",
        "        self.out_pos = (self.out_pos + 1) % self.max_length\n",
        "        return t\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = torch.zeros(self.num_channels, self.max_length, dtype=self.dtype)\n",
        "        self.in_pos = 0\n",
        "        self.out_pos = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20251f02",
      "metadata": {},
      "source": [
        "The following diagram illustrates the block architecture of WaveNet, showing how the stacked residual and skip connections form the core of the model.\n",
        "\n",
        "<div align=\"center\">\n",
        "  <img src=\"img/wavenet.png\" width=\"300\">\n",
        "</div>\n",
        "\n",
        "\n",
        "Key components and concepts in this implementation include:\n",
        "\n",
        "- **Dilated Causal Convolutions:** The core of WaveNet. These convolutions have a \"hole\" between the weights, allowing the receptive field to grow exponentially with depth without increasing the number of parameters linearly. This is crucial for capturing long-range dependencies in sequences like audio. The `dilate` function and `DilatedQueue` class likely support this mechanism.\n",
        "- **Gated Activation Units:** Similar to gates in LSTMs, these units control the flow of information through the network, allowing it to selectively remember or forget information. The `filter_convs` and `gate_convs` in the `WaveNetModel` class implement this.\n",
        "- **Residual and Skip Connections:** These connections help to train deeper networks by providing alternative paths for gradients to flow. Residual connections add the output of a dilated convolution block to its input, while skip connections contribute to the final output layer.\n",
        "- **Softmax Output:** The model outputs a probability distribution over possible next values in the audio waveform. This allows for generating diverse and realistic audio.\n",
        "- **Mu-law Compounding:** A technique used to represent audio signals with a non-linear quantization, which is particularly effective for low-amplitude signals. The `mu_law` function implements this.\n",
        "- **Fast Generation:** The `generate_fast` method likely utilizes the `DilatedQueue` to speed up the generation process by avoiding redundant computations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace484f3",
      "metadata": {
        "id": "ace484f3"
      },
      "outputs": [],
      "source": [
        "class WaveNetModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Complete Wavenet Model\n",
        "\n",
        "    Args:\n",
        "        layers (Int):               Number of layers in each block\n",
        "        blocks (Int):               Number of wavenet blocks of this model\n",
        "        dilation_channels (Int):    Number of channels for the dilated convolution\n",
        "        residual_channels (Int):    Number of channels for the residual connection\n",
        "        skip_channels (Int):        Number of channels for the skip connections\n",
        "        classes (Int):              Number of possible values each sample can have\n",
        "        output_length (Int):        Number of samples that are generated for each input\n",
        "        kernel_size (Int):          Size of the dilation kernel\n",
        "        dtype:                      Parameter type of this model\n",
        "\n",
        "    Shape:\n",
        "        - Input: :math:`(N, C_{in}, L_{in})`\n",
        "        - Output: :math:`()`\n",
        "        L should be the length of the receptive field\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 layers=10,\n",
        "                 blocks=4,\n",
        "                 dilation_channels=32,\n",
        "                 residual_channels=32,\n",
        "                 skip_channels=256,\n",
        "                 end_channels=256,\n",
        "                 classes=256,\n",
        "                 output_length=32,\n",
        "                 kernel_size=2,\n",
        "                 dtype=torch.FloatTensor,\n",
        "                 bias=False):\n",
        "\n",
        "        super(WaveNetModel, self).__init__()\n",
        "\n",
        "        self.layers = layers\n",
        "        self.blocks = blocks\n",
        "        self.dilation_channels = dilation_channels\n",
        "        self.residual_channels = residual_channels\n",
        "        self.skip_channels = skip_channels\n",
        "        self.classes = classes\n",
        "        self.kernel_size = kernel_size\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # build model\n",
        "        receptive_field = 1\n",
        "        init_dilation = 1\n",
        "\n",
        "        self.dilations = []\n",
        "        self.dilated_queues = []\n",
        "        # self.main_convs = nn.ModuleList()\n",
        "        self.filter_convs = nn.ModuleList()\n",
        "        self.gate_convs = nn.ModuleList()\n",
        "        self.residual_convs = nn.ModuleList()\n",
        "        self.skip_convs = nn.ModuleList()\n",
        "\n",
        "        # 1x1 convolution to create channels\n",
        "        self.start_conv = nn.Conv1d(in_channels=self.classes,\n",
        "                                    out_channels=residual_channels,\n",
        "                                    kernel_size=1,\n",
        "                                    bias=bias)\n",
        "\n",
        "        for b in range(blocks):\n",
        "            additional_scope = kernel_size - 1\n",
        "            new_dilation = 1\n",
        "            for i in range(layers):\n",
        "                # dilations of this layer\n",
        "                self.dilations.append((new_dilation, init_dilation))\n",
        "\n",
        "                # dilated queues for fast generation\n",
        "                self.dilated_queues.append(DilatedQueue(max_length=(kernel_size - 1) * new_dilation + 1,\n",
        "                                                        num_channels=residual_channels,\n",
        "                                                        dilation=new_dilation,\n",
        "                                                        dtype=torch.float))\n",
        "\n",
        "                # dilated convolutions\n",
        "                self.filter_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                                   out_channels=dilation_channels,\n",
        "                                                   kernel_size=kernel_size,\n",
        "                                                   bias=bias))\n",
        "\n",
        "                self.gate_convs.append(nn.Conv1d(in_channels=residual_channels,\n",
        "                                                 out_channels=dilation_channels,\n",
        "                                                 kernel_size=kernel_size,\n",
        "                                                 bias=bias))\n",
        "\n",
        "                # 1x1 convolution for residual connection\n",
        "                self.residual_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
        "                                                     out_channels=residual_channels,\n",
        "                                                     kernel_size=1,\n",
        "                                                     bias=bias))\n",
        "\n",
        "                # 1x1 convolution for skip connection\n",
        "                self.skip_convs.append(nn.Conv1d(in_channels=dilation_channels,\n",
        "                                                 out_channels=skip_channels,\n",
        "                                                 kernel_size=1,\n",
        "                                                 bias=bias))\n",
        "\n",
        "                receptive_field += additional_scope\n",
        "                additional_scope *= 2\n",
        "                init_dilation = new_dilation\n",
        "                new_dilation *= 2\n",
        "\n",
        "        self.end_conv_1 = nn.Conv1d(in_channels=skip_channels,\n",
        "                                  out_channels=end_channels,\n",
        "                                  kernel_size=1,\n",
        "                                  bias=True)\n",
        "\n",
        "        self.end_conv_2 = nn.Conv1d(in_channels=end_channels,\n",
        "                                    out_channels=classes,\n",
        "                                    kernel_size=1,\n",
        "                                    bias=True)\n",
        "\n",
        "        # self.output_length = 2 ** (layers - 1)\n",
        "        self.output_length = output_length\n",
        "        self.receptive_field = receptive_field\n",
        "\n",
        "    def wavenet(self, input, dilation_func):\n",
        "\n",
        "        x = self.start_conv(input)\n",
        "        skip = 0\n",
        "\n",
        "        # WaveNet layers\n",
        "        for i in range(self.blocks * self.layers):\n",
        "\n",
        "            #            |----------------------------------------|     *residual*\n",
        "            #            |                                        |\n",
        "            #            |    |-- conv -- tanh --|                |\n",
        "            # -> dilate -|----|                  * ----|-- 1x1 -- + -->\t*input*\n",
        "            #                 |-- conv -- sigm --|     |\n",
        "            #                                         1x1\n",
        "            #                                          |\n",
        "            # ---------------------------------------> + ------------->\t*skip*\n",
        "\n",
        "            (dilation, init_dilation) = self.dilations[i]\n",
        "\n",
        "            residual = dilation_func(x, dilation, init_dilation, i)\n",
        "\n",
        "            # dilated convolution\n",
        "            filter = self.filter_convs[i](residual)\n",
        "            filter = F.tanh(filter)\n",
        "            gate = self.gate_convs[i](residual)\n",
        "            gate = F.sigmoid(gate)\n",
        "            x = filter * gate\n",
        "\n",
        "            # parametrized skip connection\n",
        "            s = x\n",
        "            if x.size(2) != 1:\n",
        "                 s = dilate(x, 1, init_dilation=dilation)\n",
        "            s = self.skip_convs[i](s)\n",
        "            try:\n",
        "                skip = skip[:, :, -s.size(2):]\n",
        "            except:\n",
        "                skip = 0\n",
        "            skip = s + skip\n",
        "\n",
        "            x = self.residual_convs[i](x)\n",
        "            x = x + residual[:, :, (self.kernel_size - 1):]\n",
        "\n",
        "        x = F.relu(skip)\n",
        "        x = F.relu(self.end_conv_1(x))\n",
        "        x = self.end_conv_2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def wavenet_dilate(self, input, dilation, init_dilation, i):\n",
        "        x = dilate(input, dilation, init_dilation)\n",
        "        return x\n",
        "\n",
        "    def queue_dilate(self, input, dilation, init_dilation, i):\n",
        "        queue = self.dilated_queues[i]\n",
        "        queue.enqueue(input.data[0])\n",
        "        x = queue.dequeue(num_deq=self.kernel_size,\n",
        "                          dilation=dilation)\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, input):\n",
        "        x = self.wavenet(input,\n",
        "                         dilation_func=self.wavenet_dilate)\n",
        "\n",
        "        # reshape output\n",
        "        [n, c, l] = x.size()\n",
        "        l = self.output_length\n",
        "        x = x[:, :, -l:]\n",
        "        x = x.transpose(1, 2).contiguous()\n",
        "        x = x.view(n * l, c)\n",
        "        return x\n",
        "\n",
        "    def generate(self,\n",
        "                 num_samples,\n",
        "                 first_samples=None,\n",
        "                 temperature=1.):\n",
        "        self.eval()\n",
        "        if first_samples is None:\n",
        "            first_samples = self.dtype(1).zero_()\n",
        "        generated = Variable(first_samples, volatile=True)\n",
        "\n",
        "        num_pad = self.receptive_field - generated.size(0)\n",
        "        if num_pad > 0:\n",
        "            generated = constant_pad_1d(generated, self.scope, pad_start=True)\n",
        "            print(\"pad zero\")\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            input = Variable(torch.FloatTensor(1, self.classes, self.receptive_field).zero_())\n",
        "            input = input.scatter_(1, generated[-self.receptive_field:].view(1, -1, self.receptive_field), 1.)\n",
        "\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.wavenet_dilate)[:, :, -1].squeeze()\n",
        "\n",
        "            if temperature > 0:\n",
        "                x /= temperature\n",
        "                prob = F.softmax(x, dim=0)\n",
        "                prob = prob.cpu()\n",
        "                np_prob = prob.data.numpy()\n",
        "                x = np.random.choice(self.classes, p=np_prob)\n",
        "                x = Variable(torch.LongTensor([x]))#np.array([x])\n",
        "            else:\n",
        "                x = torch.max(x, 0)[1].float()\n",
        "\n",
        "            generated = torch.cat((generated, x), 0)\n",
        "\n",
        "        generated = (generated / self.classes) * 2. - 1\n",
        "        mu_gen = mu_law_expand(generated, self.classes)\n",
        "\n",
        "        self.train()\n",
        "        return mu_gen\n",
        "\n",
        "    def generate_fast(self,\n",
        "                      num_samples,\n",
        "                      first_samples=None,\n",
        "                      temperature=1.,\n",
        "                      regularize=0.,\n",
        "                      progress_callback=None,\n",
        "                      progress_interval=100):\n",
        "        self.eval()\n",
        "        if first_samples is None:\n",
        "            first_samples = torch.LongTensor(1).zero_() + (self.classes // 2)\n",
        "        first_samples = Variable(first_samples)\n",
        "\n",
        "        # reset queues\n",
        "        for queue in self.dilated_queues:\n",
        "            queue.reset()\n",
        "\n",
        "        num_given_samples = first_samples.size(0)\n",
        "        total_samples = num_given_samples + num_samples\n",
        "\n",
        "        input = Variable(torch.FloatTensor(1, self.classes, 1).zero_())\n",
        "        input = input.scatter_(1, first_samples[0:1].view(1, -1, 1), 1.)\n",
        "\n",
        "        # fill queues with given samples\n",
        "        for i in range(num_given_samples - 1):\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.queue_dilate)\n",
        "            input.zero_()\n",
        "            input = input.scatter_(1, first_samples[i + 1:i + 2].view(1, -1, 1), 1.).view(1, self.classes, 1)\n",
        "\n",
        "            # progress feedback\n",
        "            if i % progress_interval == 0:\n",
        "                if progress_callback is not None:\n",
        "                    progress_callback(i, total_samples)\n",
        "\n",
        "        # generate new samples\n",
        "        generated = np.array([])\n",
        "        regularizer = torch.pow(Variable(torch.arange(self.classes)) - self.classes / 2., 2)\n",
        "        regularizer = regularizer.squeeze() * regularize\n",
        "        tic = time.time()\n",
        "        for i in range(num_samples):\n",
        "            x = self.wavenet(input,\n",
        "                             dilation_func=self.queue_dilate).squeeze()\n",
        "\n",
        "            x -= regularizer\n",
        "\n",
        "            if temperature > 0:\n",
        "                # sample from softmax distribution\n",
        "                x /= temperature\n",
        "                prob = F.softmax(x, dim=0)\n",
        "                prob = prob.cpu()\n",
        "                np_prob = prob.data.numpy()\n",
        "                x = np.random.choice(self.classes, p=np_prob)\n",
        "                x = np.array([x])\n",
        "            else:\n",
        "                # convert to sample value\n",
        "                x = torch.max(x, 0)[1][0]\n",
        "                x = x.cpu()\n",
        "                x = x.data.numpy()\n",
        "\n",
        "            o = (x / self.classes) * 2. - 1\n",
        "            generated = np.append(generated, o)\n",
        "\n",
        "            # set new input\n",
        "            x = Variable(torch.from_numpy(x).type(torch.LongTensor))\n",
        "            input.zero_()\n",
        "            input = input.scatter_(1, x.view(1, -1, 1), 1.).view(1, self.classes, 1)\n",
        "\n",
        "            if (i+1) == 100:\n",
        "                toc = time.time()\n",
        "                print(\"one generating step does take approximately \" + str((toc - tic) * 0.01) + \" seconds)\")\n",
        "\n",
        "            # progress feedback\n",
        "            if (i + num_given_samples) % progress_interval == 0:\n",
        "                if progress_callback is not None:\n",
        "                    progress_callback(i + num_given_samples, total_samples)\n",
        "\n",
        "        self.train()\n",
        "        generated = dequantize(generated)\n",
        "        mu_gen = mu_law_expand(generated, self.classes)\n",
        "        wavfile.write(\"generated.wav\", 16000, mu_gen)\n",
        "        return  mu_gen\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea8a712",
      "metadata": {},
      "source": [
        "## Training\n",
        "\n",
        "Now, everything is ready to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa3fd20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "EPOCHS = 1\n",
        "ITERATION = 12\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "# Dataset & DataLoader\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Model\n",
        "model = WaveNetModel(layers=10,\n",
        "                     blocks=3,\n",
        "                     dilation_channels=32,\n",
        "                     residual_channels=32,\n",
        "                     skip_channels=1024,\n",
        "                     end_channels=512,\n",
        "                     output_length=1,\n",
        "                     dtype=torch.float,\n",
        "                     bias=True)\n",
        "\n",
        "model = model.cuda() if torch.cuda.is_available() else model\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Training\n",
        "\n",
        "## ITERATION\n",
        "for iter in tqdm(range(ITERATION//BATCH_SIZE +1), desc='Training ...'):\n",
        "    for x, y in loader:\n",
        "        # x: (B, classes, L), y: (B, target_length)\n",
        "        if torch.cuda.is_available():\n",
        "            x, y = x.cuda(), y.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)  # make sure out.shape = (B*L, classes)\n",
        "        y = y.view(-1)  # flatten targets\n",
        "        loss = criterion(out, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ## EPOCH\n",
        "# for epoch in tqdm(range(EPOCHS), desc='Training ...'):\n",
        "#     total_loss = 0\n",
        "#     for x, y in loader:\n",
        "#         # x: (B, classes, L), y: (B, target_length)\n",
        "#         if torch.cuda.is_available():\n",
        "#             x, y = x.cuda(), y.cuda()\n",
        "\n",
        "#         optimizer.zero_grad()\n",
        "#         out = model(x)  # make sure out.shape = (B*L, classes)\n",
        "#         y = y.view(-1)  # flatten targets\n",
        "#         loss = criterion(out, y)\n",
        "\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         total_loss += loss.item()\n",
        "\n",
        "#     print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss/len(loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d7957e0",
      "metadata": {
        "id": "5d7957e0",
        "outputId": "bca4147b-9343-482f-b7f1-caadab7a11e5"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"checkpoints/trained_model.pth\"\n",
        "\n",
        "# Load checkpoint (it might contain 'state_dict' or the whole model)\n",
        "torch.serialization.add_safe_globals([WaveNetModel])\n",
        "model.load_state_dict(torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ea7c6a5",
      "metadata": {
        "id": "4ea7c6a5",
        "outputId": "51674fbf-bbd6-4ed4-b283-005a67fc9d1f"
      },
      "outputs": [],
      "source": [
        "model.generate_fast(10000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
